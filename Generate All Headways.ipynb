{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "February 6, 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, time, datetime, csv, os\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def minSinceMidnight(t):\n",
    "    if type(t) == str:\n",
    "        tt = t.split(':')\n",
    "        msm = (int(tt[0])*60) + int(tt[1]) + (int(tt[2])/60)\n",
    "    else:\n",
    "        msm = t\n",
    "    return msm\n",
    "\n",
    "## REWROTE on 9/20 TO DEAL WITH DUPLICATE TIMES\n",
    "\n",
    "def interpolator(trip):\n",
    "    \n",
    "    lsst = list(stop_times[stop_times['trip_id'] == trip]['arrival_time'])\n",
    "    lsst = [minSinceMidnight(x) for x in lsst]\n",
    "    \n",
    "    last_scheduled_time = lsst[0]\n",
    "    lst_index = 0\n",
    "    \n",
    "    values = [last_scheduled_time]\n",
    "    \n",
    "    for x in range(1,len(lsst)):\n",
    "        #print(x)\n",
    "        if pd.isnull(lsst[x]) == False:\n",
    "            #idx = lsst.index(x)\n",
    "            #print(idx)\n",
    "            yy = list(np.linspace(last_scheduled_time, lsst[x], num=(x-lst_index+1)))\n",
    "\n",
    "            values += yy[1:]\n",
    "            last_scheduled_time = lsst[x]\n",
    "            lst_index = x\n",
    "        else:\n",
    "            pass\n",
    "    return values # returns linear interpolation of times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unchanging data\n",
    "\n",
    "# days to evaluate\n",
    "weekday = \"20190211\"\n",
    "saturday = \"20190216\"\n",
    "sunday = \"20190217\"\n",
    "\n",
    "days_of_week = {0:\"monday\", 1:\"tuesday\", 2:\"wednesday\", 3:\"thursday\", 4:\"friday\", 5:\"saturday\", 6:\"sunday\"}\n",
    "\n",
    "# modes\n",
    "bus_types = [3]\n",
    "rail_types = [0,1,2]\n",
    "ferry_types = [4]\n",
    "\n",
    "# queries to filter by various time criteria\n",
    "# strings for pd.DF.query\n",
    "\n",
    "am_peak_query = \"interpolated >= 360 and interpolated <= 600\"\n",
    "pm_peak_query = \"interpolated >= 900 and interpolated <= 1140\"\n",
    "weekday_range_query = \"interpolated >= 360 and interpolated <= 1320\"\n",
    "saturday_range_query = \"interpolated >= 480 and interpolated <= 1320\"\n",
    "sunday_range_query = \"interpolated >= 480 and interpolated <= 1320\"\n",
    "\n",
    "# defines the schema for the dict object holding headways\n",
    "\n",
    "dataHolderSpec = {\n",
    "                  \"name\":'',\n",
    "                  \"longitude\":np.NaN,\n",
    "                  \"latitude\":np.NaN,\n",
    "                0: {\n",
    "                    \"served_by\": [],\n",
    "                    \"AM Peak\": np.NaN,\n",
    "                    \"PM Peak\": np.NaN,\n",
    "                    \"Weekdays\": np.NaN,\n",
    "                    \"Saturday\": np.NaN,\n",
    "                    \"Sunday\": np.NaN\n",
    "                     },\n",
    "                 1: {\n",
    "                    \"served_by\": [],\n",
    "                    \"AM Peak\": np.NaN,\n",
    "                    \"PM Peak\": np.NaN,\n",
    "                    \"Weekdays\": np.NaN,\n",
    "                    \"Saturday\": np.NaN,\n",
    "                    \"Sunday\": np.NaN\n",
    "                     }\n",
    "                 }\n",
    "\n",
    "\n",
    "# starting index\n",
    "start_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agencies\n",
    "These cells offer a few different ways to define the agency to be analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_agencies = sorted(list(os.walk('gtfs'))[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(columns=[\"agency\",\"path\", \"error\"])\n",
    "\n",
    "for agency in list_of_agencies:\n",
    "    path = \"gtfs/\" + agency + \"/\"\n",
    "    #print(path)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    \n",
    "    # LOAD GTFS DATA\n",
    "    agency_name = pd.read_csv(path + 'agency.txt')['agency_name'][0]\n",
    "    agency_name = agency_name.replace(\"/\",\"-\")\n",
    "    print(agency_name, list_of_agencies.index(agency)+1, \"of\", len(list_of_agencies))\n",
    "\n",
    "    trips = pd.read_csv(path + 'trips.txt', dtype={\"trip_id\":str, \"route_id\":str})\n",
    "    #print(len(trips), \"trips\")\n",
    "\n",
    "    routes = pd.read_csv(path + 'routes.txt',dtype={\"route_id\":str})\n",
    "    #print(len(routes), \"routes\")\n",
    "\n",
    "    stops = pd.read_csv(path + 'stops.txt', dtype={\"stop_id\":str})\n",
    "    #print(len(stops), \"stops\")\n",
    "\n",
    "    stop_times = pd.read_csv(path + 'stop_times.txt', dtype={\"stop_id\":str})\n",
    "    #print(len(stop_times), \"stop times\")\n",
    "\n",
    "    if os.path.isfile(path + \"calendar.txt\"):\n",
    "        calendar = pd.read_csv(path + \"calendar.txt\")\n",
    "        #print(len(calendar), \"schedules\")\n",
    "        calend = True\n",
    "        \n",
    "        if sum(calendar[[\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]].sum()) == 0:\n",
    "            # build a calendar\n",
    "            calend = True\n",
    "            \n",
    "            calendar_dates= pd.read_csv(path + \"calendar_dates.txt\")\n",
    "            calendar_dates[\"day_of_week\"] = [datetime.datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])).weekday() for x in calendar_dates[\"date\"]]\n",
    "\n",
    "            calendar = pd.crosstab(calendar_dates[\"service_id\"], calendar_dates[\"day_of_week\"]).rename(days_of_week, axis=1).reset_index()\n",
    "\n",
    "            service_id_df = []\n",
    "\n",
    "            for service_id in calendar_dates[\"service_id\"].unique():\n",
    "\n",
    "                tempServiceID_df = calendar_dates[calendar_dates[\"service_id\"] == service_id]\n",
    "                minDate = tempServiceID_df[\"date\"].min()\n",
    "                maxDate = tempServiceID_df[\"date\"].max()\n",
    "                service_id_df.append(pd.DataFrame([[service_id, minDate,maxDate]], columns=[\"service_id\", \"start_date\",\"end_date\"]))\n",
    "    \n",
    "    \n",
    "            calendar = calendar.merge(pd.concat(service_id_df, ignore_index=True))\n",
    "            for date in [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]:\n",
    "                calendar[date] = [1 if x > 0 else 0 for x in calendar[date]]\n",
    "    else:\n",
    "        calend = False\n",
    "\n",
    "    calendar_dates = pd.read_csv(path + \"calendar_dates.txt\")\n",
    "    #print(len(calendar_dates), \"exception dates\")\n",
    "\n",
    "    # READ AND DETERMINE CALENDARS\n",
    "\n",
    "    # create dictionary object with exceptions dates for different types of service\n",
    "\n",
    "    exceptions = {}\n",
    "    \n",
    "    for x in range(len(calendar_dates)):\n",
    "        exception_date = str(calendar_dates['date'][x])\n",
    "        exception_service_id = calendar_dates['service_id'][x]\n",
    "        exception_type = calendar_dates['exception_type'][x]\n",
    "\n",
    "        if exception_date not in exceptions:\n",
    "            exceptions[exception_date] = {1:[],2:[]}\n",
    "            exceptions[exception_date][exception_type].append(exception_service_id)\n",
    "        else:\n",
    "            exceptions[exception_date][exception_type].append(exception_service_id)\n",
    "\n",
    "    # GENERATES service_ids IN USE ON SPECIFIED DAY OF WEEK\n",
    "    # ALSO ENSURES THAT SCHEDULES ARE ACTIVE DURING SPECIFIED TIME FRAME\n",
    "\n",
    "    weekday_day = days_of_week[datetime.datetime(int(weekday[:4]), int(weekday[4:6]), int(weekday[6:])).weekday()]\n",
    "    saturday_day = days_of_week[datetime.datetime(int(saturday[:4]), int(saturday[4:6]), int(saturday[6:])).weekday()]\n",
    "    sunday_day = days_of_week[datetime.datetime(int(sunday[:4]), int(sunday[4:6]), int(sunday[6:])).weekday()]\n",
    "\n",
    "    # this if-else statement checks to make sure that calendar.txt file is up to date\n",
    "    # if none of the service_id are currently active, we take them anyway\n",
    "    # otherwise, we filter out any inactive service_id\n",
    "    \n",
    "    # only use start dates for service filtering\n",
    "    weekday_query = \"(%s == 1) & (%s >= start_date)\" % (weekday_day, weekday)\n",
    "    saturday_query = \"(%s == 1) & (%s >= start_date)\" % (saturday_day, saturday)\n",
    "    sunday_query = \"(%s == 1) & (%s >= start_date)\" % (sunday_day, sunday)\n",
    "    \n",
    "    \n",
    "    '''try:\n",
    "        if len([x for x in list(calendar['end_date']) if x > np.min([int(weekday), int(saturday), int(sunday)])]) == 0:\n",
    "            print(\"just start date\")\n",
    "            weekday_query = \"(%s == 1) & (%s >= start_date)\" % (weekday_day, weekday)\n",
    "            saturday_query = \"(%s == 1) & (%s >= start_date)\" % (saturday_day, saturday)\n",
    "            sunday_query = \"(%s == 1) & (%s >= start_date)\" % (sunday_day, sunday)\n",
    "        else:\n",
    "            print(\"end date too\")\n",
    "            weekday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (weekday_day, weekday, weekday)\n",
    "            saturday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (saturday_day, saturday, saturday)\n",
    "            sunday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (sunday_day, sunday, sunday)\n",
    "    except NameError:\n",
    "        # the Long Beach Exception\n",
    "        weekday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (weekday_day, weekday, weekday)\n",
    "        saturday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (saturday_day, saturday, saturday)\n",
    "        sunday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (sunday_day, sunday, sunday)'''\n",
    "\n",
    "    # LISTS OF SERVICE_IDs FOR USE IN ANALYSIS\n",
    "    if calend == True:\n",
    "        weekday_service = list(calendar.query(weekday_query)['service_id'])\n",
    "        saturday_service = list(calendar.query(saturday_query)['service_id'])\n",
    "        sunday_service = list(calendar.query(sunday_query)['service_id'])\n",
    "    elif calend == False:\n",
    "        weekday_service = exceptions[weekday][1]\n",
    "        saturday_service = exceptions[saturday][1]\n",
    "        sunday_service = exceptions[sunday][1]\n",
    "    # create dictionary object to hold information about stops\n",
    "    # dictionary will be used for quick access\n",
    "    \n",
    "    if len(weekday_service) == 0 :\n",
    "        if sum(calendar[weekday_day]) == 0:\n",
    "            #print(\"no agency weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"no agency weekday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing weekday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    if len(saturday_service) == 0:\n",
    "        if sum(calendar[\"saturday\"]) == 0:\n",
    "            #print(\"no agency saturday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path, \"no agency saturday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\") \n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing saturday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    if len(sunday_service) == 0:\n",
    "        if sum(calendar[\"sunday\"]) == 0:\n",
    "            #print(\"no agency sunday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"no agency sunday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing sunday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    \n",
    "    # create the fully-formed stops file\n",
    "    if \"direction_id\" in trips.columns:\n",
    "        trip_info = trips[[\"route_id\", \"service_id\", \"trip_id\", \"direction_id\"]]\n",
    "    else:\n",
    "        trip_info = trips[[\"route_id\", \"service_id\", \"trip_id\"]]\n",
    "    route_info = routes[['route_id', 'route_type']]\n",
    "\n",
    "    trip_route_merged = trip_info.merge(route_info, on=\"route_id\")\n",
    "\n",
    "    # reorder columns\n",
    "    if \"direction_id\" in trips.columns:\n",
    "        trip_route_merged = trip_route_merged[[\"trip_id\", \"route_id\", \"service_id\", \"direction_id\", \"route_type\"]]\n",
    "    else:\n",
    "        trip_route_merged = trip_route_merged[[\"trip_id\", \"route_id\", \"service_id\", \"route_type\"]]\n",
    "\n",
    "    if trips['trip_id'].dtype == object:\n",
    "\n",
    "        # create a string version of trip_id\n",
    "        stop_times['trip_id_str'] = stop_times['trip_id'].astype(\"str\")\n",
    "\n",
    "        # merge trip info onto stop_times\n",
    "        stop_times_merged = stop_times[[\"trip_id_str\", 'arrival_time', 'stop_id']].merge(trip_route_merged, right_on=\"trip_id\", left_on=\"trip_id_str\", how='left')\n",
    "\n",
    "    else:\n",
    "\n",
    "        stop_times_merged = stop_times[[\"trip_id\", 'arrival_time', 'stop_id']].merge(trip_route_merged, right_on=\"trip_id\", left_on=\"trip_id\", how='left')\n",
    "\n",
    "    # calculate a interpolated (float) time for future analysis\n",
    "\n",
    "\n",
    "    if (sum(pd.isnull(stop_times_merged['arrival_time']))/len(stop_times_merged)) > 0.05:\n",
    "        agency_trips = list(stop_times.drop_duplicates('trip_id')['trip_id'])\n",
    "        allInterpolatedTimes = []\n",
    "        for trip in agency_trips:\n",
    "            allInterpolatedTimes += interpolator(trip)\n",
    "        stop_times_merged['interpolated'] = allInterpolatedTimes\n",
    "        #print(\"INTERP-O-LATED\")\n",
    "    else:\n",
    "        stop_times_merged['interpolated'] = [minSinceMidnight(x) for x in stop_times_merged['arrival_time']]\n",
    "\n",
    "    #stop_times_merged.head()\n",
    "     \n",
    "    # Dump all rail and ferry stops into their own CSVs. Remove rail and ferry stops from the <i>stop_times_merged</i> dataset.\n",
    "    \n",
    "    rail_times_merged = stop_times_merged.query(\"route_type in [0,1,2]\")\n",
    "\n",
    "    if len(rail_times_merged) > 0:\n",
    "        rail_stops = pd.DataFrame(rail_times_merged['stop_id'].unique(), columns=['stop_id'])\n",
    "\n",
    "        if stops['stop_id'].dtype != object:\n",
    "            rail_stops = rail_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], on=\"stop_id\")\n",
    "        else:\n",
    "            rail_stops['stop_id_str'] = rail_stops['stop_id'].astype('str')\n",
    "            rail_stops.drop(\"stop_id\", axis=1, inplace=True)\n",
    "            rail_stops = rail_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], left_on=\"stop_id_str\", right_on=\"stop_id\")\n",
    "            rail_stops.drop(\"stop_id_str\", axis=1, inplace=True)\n",
    "        rail_stops.to_csv(\"output/v2/rail/\" + agency_name + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ferry_times_merged = stop_times_merged.query(\"route_type in [4]\")\n",
    "\n",
    "    if len(ferry_times_merged) > 0:\n",
    "        ferry_stops = pd.DataFrame(ferry_times_merged['stop_id'].unique(), columns=['stop_id'])\n",
    "\n",
    "        if stops['stop_id'].dtype != object:\n",
    "            ferry_stops = ferry_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], on=\"stop_id\")\n",
    "        else:\n",
    "            ferry_stops['stop_id_str'] = ferry_stops['stop_id'].astype('str')\n",
    "            ferry_stops.drop(\"stop_id\", axis=1, inplace=True)\n",
    "            ferry_stops = ferry_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], left_on=\"stop_id_str\", right_on=\"stop_id\")\n",
    "            ferry_stops.drop(\"stop_id_str\", axis=1, inplace=True)\n",
    "        ferry_stops.to_csv(\"output/v2/ferry/\" + agency_name + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    stop_times_merged = stop_times_merged.query(\"route_type == 3\")\n",
    "    \n",
    "    # WEEKDAY\n",
    "\n",
    "    if weekday in exceptions:\n",
    "        weekday_stops = [weekday_service.remove(x) for x in exceptions[weekday][2] if x in weekday_service] + exceptions[weekday][1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weekday_stops = stop_times_merged[stop_times_merged['service_id'].isin(weekday_service)]\n",
    "    weekday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)\n",
    "    #print(weekday_stops.shape)\n",
    "\n",
    "    # SATURDAY\n",
    "\n",
    "    if saturday in exceptions:\n",
    "        saturday_service = [saturday_service.remove(x) for x in exceptions[saturday][2]] + exceptions[saturday][1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    saturday_stops = stop_times_merged[stop_times_merged['service_id'].isin(saturday_service)]\n",
    "    saturday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)\n",
    "    #print(saturday_stops.shape)\n",
    "    \n",
    "    # SUNDAY\n",
    "\n",
    "    if sunday in exceptions:\n",
    "        sunday_service = [sunday_service.remove(x) for x in exceptions[sunday][2]] + exceptions[sunday][1]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    sunday_stops = stop_times_merged[stop_times_merged['service_id'].isin(sunday_service)]\n",
    "    sunday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # OUTPUT AND ANALYSIS\n",
    "    output_path = \"output/v2/bus/\" + agency_name + \".csv\"\n",
    "\n",
    "    if len(stop_times_merged) > 0:\n",
    "        headways = pd.DataFrame(data=stops['stop_id'])\n",
    "        #headways[\"stop_id\"] = headways[\"stop_id\"].astype(str)\n",
    "\n",
    "        minutes = 60 * 4\n",
    "        \n",
    "\n",
    "        queries = [am_peak_query, pm_peak_query, weekday_range_query, saturday_range_query, sunday_range_query]\n",
    "        periods = ['am_pk', 'pm_pk', 'wkdy', 'sat', 'sun']\n",
    "        minute_ranges = [(600-360), (1140-900), (1320-360), (1320-480), (1320-480)]\n",
    "        dfs = [weekday_stops, weekday_stops, weekday_stops, saturday_stops, sunday_stops]\n",
    "\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            # conditional parameters\n",
    "            df = dfs[i]\n",
    "            minutes = minute_ranges[i]\n",
    "            query = queries[i]\n",
    "\n",
    "            if \"direction_id\" in df.columns:\n",
    "                for direction in [0,1]:\n",
    "                    results = pd.DataFrame(minutes / df[df['direction_id']==direction].query(query)['stop_id'].value_counts())\n",
    "                    results.reset_index(inplace=True)\n",
    "                    results.rename(columns={\"stop_id\":periods[i]}, inplace=True)\n",
    "                    results.rename(columns={\"index\":\"stop_id\"}, inplace=True)\n",
    "\n",
    "                    results[\"stop_id\"] = results[\"stop_id\"].astype(str)\n",
    "                    headways = headways.merge(results, on=\"stop_id\", suffixes=[\"_dir0\", \"_dir1\"], how='left')\n",
    "                \n",
    "            else:\n",
    "                results = pd.DataFrame(minutes / df.query(query)['stop_id'].value_counts())\n",
    "                results.reset_index(inplace=True)\n",
    "                results.rename(columns={\"stop_id\":periods[i]}, inplace=True)\n",
    "                results.rename(columns={\"index\":\"stop_id\"}, inplace=True)\n",
    "\n",
    "                results[\"stop_id\"] = results[\"stop_id\"].astype(str)\n",
    "                headways = headways.merge(results, on=\"stop_id\", how='left')\n",
    "        \n",
    "        if \"am_pk_dir0\" in headways.columns:\n",
    "            headways[\"am_pk\"] = headways[[\"am_pk_dir0\", \"am_pk_dir1\"]].min(axis=1)\n",
    "            headways[\"pm_pk\"] = headways[[\"pm_pk_dir0\", \"pm_pk_dir1\"]].min(axis=1)\n",
    "\n",
    "            headways[\"wkdy\"] = headways[[\"wkdy_dir0\", \"wkdy_dir1\"]].min(axis=1)\n",
    "            headways[\"sat\"] = headways[[\"sat_dir0\", \"sat_dir1\"]].min(axis=1)\n",
    "            headways[\"sun\"] = headways[[\"sun_dir0\", \"sun_dir1\"]].min(axis=1)\n",
    "            headways = headways[['stop_id', \"am_pk\", \"pm_pk\", \"wkdy\", \"sat\", \"sun\"]] \n",
    "        else:\n",
    "            pass\n",
    "        #stops[\"stop_id\"] = stops[\"stop_id\"].astype(str)\n",
    "        output = stops[[\"stop_id\", \"stop_name\", \"stop_lon\", \"stop_lat\"]].merge(headways, on=\"stop_id\", how=\"outer\")\n",
    "\n",
    "        output.to_csv(output_path)\n",
    "    else:\n",
    "        print(\"No bus data.\")\n",
    "    \n",
    "    hqt_filter = \"(am_pk <= 15) & (pm_pk <= 15)\"\n",
    "    hqt_filter += \" & (wkdy <= 20) & \"\n",
    "    hqt_filter += \"(sat <= 30) & (sun <= 30)\"\n",
    "    \n",
    "    \n",
    "    hqt = output.query(hqt_filter)\n",
    "\n",
    "    if len(hqt) > 0:\n",
    "        hqt.to_csv(\"output/v2/bus-hqt/\" + agency_name + \" (HQT).csv\")\n",
    "    else:\n",
    "        pass\n",
    "# output.head()\n",
    "\n",
    "errors_path = \"errors-{}-{}-{}.csv\".format(weekday, saturday, sunday)\n",
    "errors.to_csv(errors_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsLog = pd.read_csv(errors_path)\n",
    "\n",
    "for ag in errorsLog['agency'].unique():\n",
    "    print(ag)\n",
    "errorsLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorsLog[errorsLog[\"error\"] == \"no agency weekday service\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agency in list_of_agencies:\n",
    "    path = \"gtfs/\" + agency + \"/calendar.txt\"\n",
    "    if os.path.isfile(path) == False:\n",
    "        print(agency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency = \"gtfs/gold-coast-transit--339/\"\n",
    "path = agency + \"calendar.txt\"\n",
    "\n",
    "calendar = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(calendar[[\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agency = \"gtfs/sacramento-regional-transit--161/\"\n",
    "path = agency + \"calendar.txt\"\n",
    "\n",
    "calendar = pd.read_csv(path)\n",
    "\n",
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_dates= pd.read_csv(agency + \"calendar_dates.txt\")\n",
    "calendar_dates[\"day_of_week\"] = [datetime.datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])).weekday() for x in calendar_dates[\"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendarBYO = pd.crosstab(calendar_dates[\"service_id\"], calendar_dates[\"day_of_week\"]).rename(days_of_week, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendarBYO.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_dates= pd.read_csv(\"gtfs/\" + agency + \"/calendar_dates.txt\")\n",
    "calendar_dates[\"day_of_week\"] = [datetime.datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])).weekday() for x in calendar_dates[\"date\"]]\n",
    "\n",
    "calendar = pd.crosstab(calendar_dates[\"service_id\"], calendar_dates[\"day_of_week\"]).rename(days_of_week, axis=1).reset_index()\n",
    "\n",
    "service_id_df = []\n",
    "\n",
    "for service_id in calendar_dates[\"service_id\"].unique():\n",
    "    \n",
    "    tempServiceID_df = calendar_dates[calendar_dates[\"service_id\"] == service_id]\n",
    "    minDate = tempServiceID_df[\"date\"].min()\n",
    "    maxDate = tempServiceID_df[\"date\"].max()\n",
    "    service_id_df.append(pd.DataFrame([[service_id, minDate,maxDate]], columns=[\"service_id\", \"start_date\",\"end_date\"]))\n",
    "    \n",
    "    \n",
    "calendar = calendar.merge(pd.concat(service_id_df, ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([[service_id, minDate]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# befre changing to only use start date to consider schedule\n",
    "errorsLog.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldErrorsLog = errorsLog.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDays(testday):\n",
    "    entered = str(testday)\n",
    "    yr = int(entered[:4])\n",
    "    mo = int(entered[4:6])\n",
    "    d = int(entered[6:])\n",
    "    day_of_week =  datetime.datetime(yr, mo, d).weekday()\n",
    "    if day_of_week == 5:\n",
    "        _saturday = testday\n",
    "        _sunday = dayAdder(yr,mo,d,1)\n",
    "        _weekday = dayAdder(yr,mo,d,2)\n",
    "    elif day_of_week == 6:\n",
    "        _sunday = testday\n",
    "        _saturday = dayAdder(yr,mo,d,6)\n",
    "        _weekday = dayAdder(yr,mo,d,1)\n",
    "    else:\n",
    "        _weekday = testday\n",
    "        _saturday = dayAdder(yr,mo,d,(5-day_of_week))\n",
    "        _sunday = dayAdder(yr,mo,d,(6-day_of_week))\n",
    "    return str(_weekday),str(_saturday), str(_sunday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dayAdder(y,mo,d, days2add):\n",
    "    if mo in [1,3,5,7,8,10]:\n",
    "        if d+days2add > 31:\n",
    "            mo = mo + 1\n",
    "            d = (d + days2add) - 31\n",
    "        else:\n",
    "            d = d + days2add\n",
    "    elif mo in [4,6,9,11]:\n",
    "        if d+days2add > 30:\n",
    "            mo = mo + 1\n",
    "            d = (d + days2add) - 30\n",
    "        else:\n",
    "            d = d + days2add\n",
    "    elif mo == 2:\n",
    "        if y % 4 == 0:\n",
    "            if d + days2add > 29:\n",
    "                mo = 3\n",
    "                d = (d + days2add) - 29\n",
    "            else:\n",
    "                d = d + days2add\n",
    "        else:\n",
    "            if d + days2add > 28:\n",
    "                mo = 3\n",
    "                d = (d + days2add) - 28\n",
    "            else:\n",
    "                d = d + days2add\n",
    "    elif mo == 12:\n",
    "        if d + days2add > 31:\n",
    "            y += 1\n",
    "            mo = 1\n",
    "            d = (d + days2add) - 31\n",
    "        else:\n",
    "            d = d + days2add\n",
    "    return str(y)+str(mo).zfill(2)+str(d).zfill(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"dog\".zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayAdder(2019,12,31,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_dates[\"date\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = pd.DataFrame(columns=[\"agency\",\"path\", \"error\"])\n",
    "\n",
    "for agency in [\"gold-coast-transit--339\", \"sacramento-regional-transit--161\"]:\n",
    "    path = \"gtfs/\" + agency + \"/\"\n",
    "    #print(path)\n",
    "    \n",
    "    startTime = time.time()\n",
    "    \n",
    "    \n",
    "    # LOAD GTFS DATA\n",
    "    agency_name = pd.read_csv(path + 'agency.txt')['agency_name'][0]\n",
    "    agency_name = agency_name.replace(\"/\",\"-\")\n",
    "    print(agency_name, list_of_agencies.index(agency)+1, \"of\", len(list_of_agencies))\n",
    "\n",
    "    trips = pd.read_csv(path + 'trips.txt', dtype={\"trip_id\":str, \"route_id\":str})\n",
    "    #print(len(trips), \"trips\")\n",
    "\n",
    "    routes = pd.read_csv(path + 'routes.txt',dtype={\"route_id\":str})\n",
    "    #print(len(routes), \"routes\")\n",
    "\n",
    "    stops = pd.read_csv(path + 'stops.txt', dtype={\"stop_id\":str})\n",
    "    #print(len(stops), \"stops\")\n",
    "\n",
    "    stop_times = pd.read_csv(path + 'stop_times.txt', dtype={\"stop_id\":str})\n",
    "    #print(len(stop_times), \"stop times\")\n",
    "\n",
    "    if os.path.isfile(path + \"calendar.txt\"):\n",
    "        calendar = pd.read_csv(path + \"calendar.txt\")\n",
    "        #print(len(calendar), \"schedules\")\n",
    "        calend = True\n",
    "        \n",
    "        if sum(calendar[[\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]].sum()) == 0:\n",
    "            # build a calendar\n",
    "            calend = True\n",
    "            \n",
    "            calendar_dates= pd.read_csv(path + \"calendar_dates.txt\")\n",
    "            calendar_dates[\"day_of_week\"] = [datetime.datetime(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])).weekday() for x in calendar_dates[\"date\"]]\n",
    "\n",
    "            calendar = pd.crosstab(calendar_dates[\"service_id\"], calendar_dates[\"day_of_week\"]).rename(days_of_week, axis=1).reset_index()\n",
    "\n",
    "            service_id_df = []\n",
    "\n",
    "            for service_id in calendar_dates[\"service_id\"].unique():\n",
    "\n",
    "                tempServiceID_df = calendar_dates[calendar_dates[\"service_id\"] == service_id]\n",
    "                minDate = tempServiceID_df[\"date\"].min()\n",
    "                maxDate = tempServiceID_df[\"date\"].max()\n",
    "                service_id_df.append(pd.DataFrame([[service_id, minDate,maxDate]], columns=[\"service_id\", \"start_date\",\"end_date\"]))\n",
    "    \n",
    "    \n",
    "            calendar = calendar.merge(pd.concat(service_id_df, ignore_index=True))\n",
    "            for date in [\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\"]:\n",
    "                calendar[date] = [1 if x > 0 else 0 for x in calendar[date]]\n",
    "    else:\n",
    "        calend = False\n",
    "\n",
    "    calendar_dates = pd.read_csv(path + \"calendar_dates.txt\")\n",
    "    #print(len(calendar_dates), \"exception dates\")\n",
    "\n",
    "    # READ AND DETERMINE CALENDARS\n",
    "\n",
    "    # create dictionary object with exceptions dates for different types of service\n",
    "\n",
    "    exceptions = {}\n",
    "    \n",
    "    for x in range(len(calendar_dates)):\n",
    "        exception_date = str(calendar_dates['date'][x])\n",
    "        exception_service_id = calendar_dates['service_id'][x]\n",
    "        exception_type = calendar_dates['exception_type'][x]\n",
    "\n",
    "        if exception_date not in exceptions:\n",
    "            exceptions[exception_date] = {1:[],2:[]}\n",
    "            exceptions[exception_date][exception_type].append(exception_service_id)\n",
    "        else:\n",
    "            exceptions[exception_date][exception_type].append(exception_service_id)\n",
    "\n",
    "    # GENERATES service_ids IN USE ON SPECIFIED DAY OF WEEK\n",
    "    # ALSO ENSURES THAT SCHEDULES ARE ACTIVE DURING SPECIFIED TIME FRAME\n",
    "\n",
    "    weekday_day = days_of_week[datetime.datetime(int(weekday[:4]), int(weekday[4:6]), int(weekday[6:])).weekday()]\n",
    "    saturday_day = days_of_week[datetime.datetime(int(saturday[:4]), int(saturday[4:6]), int(saturday[6:])).weekday()]\n",
    "    sunday_day = days_of_week[datetime.datetime(int(sunday[:4]), int(sunday[4:6]), int(sunday[6:])).weekday()]\n",
    "\n",
    "    # this if-else statement checks to make sure that calendar.txt file is up to date\n",
    "    # if none of the service_id are currently active, we take them anyway\n",
    "    # otherwise, we filter out any inactive service_id\n",
    "    \n",
    "    # only use start dates for service filtering\n",
    "    weekday_query = \"(%s == 1) & (%s >= start_date)\" % (weekday_day, weekday)\n",
    "    saturday_query = \"(%s == 1) & (%s >= start_date)\" % (saturday_day, saturday)\n",
    "    sunday_query = \"(%s == 1) & (%s >= start_date)\" % (sunday_day, sunday)\n",
    "    \n",
    "    \n",
    "    '''try:\n",
    "        if len([x for x in list(calendar['end_date']) if x > np.min([int(weekday), int(saturday), int(sunday)])]) == 0:\n",
    "            print(\"just start date\")\n",
    "            weekday_query = \"(%s == 1) & (%s >= start_date)\" % (weekday_day, weekday)\n",
    "            saturday_query = \"(%s == 1) & (%s >= start_date)\" % (saturday_day, saturday)\n",
    "            sunday_query = \"(%s == 1) & (%s >= start_date)\" % (sunday_day, sunday)\n",
    "        else:\n",
    "            print(\"end date too\")\n",
    "            weekday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (weekday_day, weekday, weekday)\n",
    "            saturday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (saturday_day, saturday, saturday)\n",
    "            sunday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (sunday_day, sunday, sunday)\n",
    "    except NameError:\n",
    "        # the Long Beach Exception\n",
    "        weekday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (weekday_day, weekday, weekday)\n",
    "        saturday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (saturday_day, saturday, saturday)\n",
    "        sunday_query = \"(%s == 1) & (%s >= start_date) & (%s <= end_date)\" % (sunday_day, sunday, sunday)'''\n",
    "\n",
    "    # LISTS OF SERVICE_IDs FOR USE IN ANALYSIS\n",
    "    if calend == True:\n",
    "        weekday_service = list(calendar.query(weekday_query)['service_id'])\n",
    "        saturday_service = list(calendar.query(saturday_query)['service_id'])\n",
    "        sunday_service = list(calendar.query(sunday_query)['service_id'])\n",
    "    elif calend == False:\n",
    "        weekday_service = exceptions[weekday][1]\n",
    "        saturday_service = exceptions[saturday][1]\n",
    "        sunday_service = exceptions[sunday][1]\n",
    "    # create dictionary object to hold information about stops\n",
    "    # dictionary will be used for quick access\n",
    "    \n",
    "    if len(weekday_service) == 0 :\n",
    "        if sum(calendar[weekday_day]) == 0:\n",
    "            #print(\"no agency weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"no agency weekday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing weekday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    if len(saturday_service) == 0:\n",
    "        if sum(calendar[\"saturday\"]) == 0:\n",
    "            #print(\"no agency saturday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path, \"no agency saturday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\") \n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing saturday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    if len(sunday_service) == 0:\n",
    "        if sum(calendar[\"sunday\"]) == 0:\n",
    "            #print(\"no agency sunday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"no agency sunday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "        else:\n",
    "            #print(\"missing weekday service\")\n",
    "            errors = errors.append(pd.DataFrame([[agency_name, path,\"missing sunday service\"]], columns=[\"agency\", \"path\",\"error\"]), ignore_index=True)\n",
    "    \n",
    "    # create the fully-formed stops file\n",
    "    if \"direction_id\" in trips.columns:\n",
    "        trip_info = trips[[\"route_id\", \"service_id\", \"trip_id\", \"direction_id\"]]\n",
    "    else:\n",
    "        trip_info = trips[[\"route_id\", \"service_id\", \"trip_id\"]]\n",
    "    route_info = routes[['route_id', 'route_type']]\n",
    "\n",
    "    trip_route_merged = trip_info.merge(route_info, on=\"route_id\")\n",
    "\n",
    "    # reorder columns\n",
    "    if \"direction_id\" in trips.columns:\n",
    "        trip_route_merged = trip_route_merged[[\"trip_id\", \"route_id\", \"service_id\", \"direction_id\", \"route_type\"]]\n",
    "    else:\n",
    "        trip_route_merged = trip_route_merged[[\"trip_id\", \"route_id\", \"service_id\", \"route_type\"]]\n",
    "\n",
    "    if trips['trip_id'].dtype == object:\n",
    "\n",
    "        # create a string version of trip_id\n",
    "        stop_times['trip_id_str'] = stop_times['trip_id'].astype(\"str\")\n",
    "\n",
    "        # merge trip info onto stop_times\n",
    "        stop_times_merged = stop_times[[\"trip_id_str\", 'arrival_time', 'stop_id']].merge(trip_route_merged, right_on=\"trip_id\", left_on=\"trip_id_str\", how='left')\n",
    "\n",
    "    else:\n",
    "\n",
    "        stop_times_merged = stop_times[[\"trip_id\", 'arrival_time', 'stop_id']].merge(trip_route_merged, right_on=\"trip_id\", left_on=\"trip_id\", how='left')\n",
    "\n",
    "    # calculate a interpolated (float) time for future analysis\n",
    "\n",
    "\n",
    "    if (sum(pd.isnull(stop_times_merged['arrival_time']))/len(stop_times_merged)) > 0.05:\n",
    "        agency_trips = list(stop_times.drop_duplicates('trip_id')['trip_id'])\n",
    "        allInterpolatedTimes = []\n",
    "        for trip in agency_trips:\n",
    "            allInterpolatedTimes += interpolator(trip)\n",
    "        stop_times_merged['interpolated'] = allInterpolatedTimes\n",
    "        #print(\"INTERP-O-LATED\")\n",
    "    else:\n",
    "        stop_times_merged['interpolated'] = [minSinceMidnight(x) for x in stop_times_merged['arrival_time']]\n",
    "\n",
    "    #stop_times_merged.head()\n",
    "     \n",
    "    # Dump all rail and ferry stops into their own CSVs. Remove rail and ferry stops from the <i>stop_times_merged</i> dataset.\n",
    "    \n",
    "    rail_times_merged = stop_times_merged.query(\"route_type in [0,1,2]\")\n",
    "\n",
    "    if len(rail_times_merged) > 0:\n",
    "        rail_stops = pd.DataFrame(rail_times_merged['stop_id'].unique(), columns=['stop_id'])\n",
    "\n",
    "        if stops['stop_id'].dtype != object:\n",
    "            rail_stops = rail_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], on=\"stop_id\")\n",
    "        else:\n",
    "            rail_stops['stop_id_str'] = rail_stops['stop_id'].astype('str')\n",
    "            rail_stops.drop(\"stop_id\", axis=1, inplace=True)\n",
    "            rail_stops = rail_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], left_on=\"stop_id_str\", right_on=\"stop_id\")\n",
    "            rail_stops.drop(\"stop_id_str\", axis=1, inplace=True)\n",
    "        rail_stops.to_csv(\"output/v2/rail/\" + agency_name + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ferry_times_merged = stop_times_merged.query(\"route_type in [4]\")\n",
    "\n",
    "    if len(ferry_times_merged) > 0:\n",
    "        ferry_stops = pd.DataFrame(ferry_times_merged['stop_id'].unique(), columns=['stop_id'])\n",
    "\n",
    "        if stops['stop_id'].dtype != object:\n",
    "            ferry_stops = ferry_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], on=\"stop_id\")\n",
    "        else:\n",
    "            ferry_stops['stop_id_str'] = ferry_stops['stop_id'].astype('str')\n",
    "            ferry_stops.drop(\"stop_id\", axis=1, inplace=True)\n",
    "            ferry_stops = ferry_stops.merge(stops[['stop_id', 'stop_name', 'stop_lon', 'stop_lat']], left_on=\"stop_id_str\", right_on=\"stop_id\")\n",
    "            ferry_stops.drop(\"stop_id_str\", axis=1, inplace=True)\n",
    "        ferry_stops.to_csv(\"output/v2/ferry/\" + agency_name + \".csv\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    stop_times_merged = stop_times_merged.query(\"route_type == 3\")\n",
    "    \n",
    "    # WEEKDAY\n",
    "\n",
    "    if weekday in exceptions:\n",
    "        weekday_stops = [weekday_service.remove(x) for x in exceptions[weekday][2] if x in weekday_service] + exceptions[weekday][1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    weekday_stops = stop_times_merged[stop_times_merged['service_id'].isin(weekday_service)]\n",
    "    weekday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)\n",
    "    #print(weekday_stops.shape)\n",
    "\n",
    "    # SATURDAY\n",
    "\n",
    "    if saturday in exceptions:\n",
    "        saturday_service = [saturday_service.remove(x) for x in exceptions[saturday][2]] + exceptions[saturday][1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    saturday_stops = stop_times_merged[stop_times_merged['service_id'].isin(saturday_service)]\n",
    "    saturday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)\n",
    "    #print(saturday_stops.shape)\n",
    "    \n",
    "    # SUNDAY\n",
    "\n",
    "    if sunday in exceptions:\n",
    "        sunday_service = [sunday_service.remove(x) for x in exceptions[sunday][2]] + exceptions[sunday][1]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    sunday_stops = stop_times_merged[stop_times_merged['service_id'].isin(sunday_service)]\n",
    "    print(sunday_stops.shape)\n",
    "    sunday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"], inplace=True)\n",
    "    print(sunday_stops.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # OUTPUT AND ANALYSIS\n",
    "    output_path = \"output/v2/bus/\" + agency_name + \".csv\"\n",
    "\n",
    "    if len(stop_times_merged) > 0:\n",
    "        headways = pd.DataFrame(data=stops['stop_id'])\n",
    "        #headways[\"stop_id\"] = headways[\"stop_id\"].astype(str)\n",
    "\n",
    "        minutes = 60 * 4\n",
    "        \n",
    "\n",
    "        queries = [am_peak_query, pm_peak_query, weekday_range_query, saturday_range_query, sunday_range_query]\n",
    "        periods = ['am_pk', 'pm_pk', 'wkdy', 'sat', 'sun']\n",
    "        minute_ranges = [(600-360), (1140-900), (1320-360), (1320-480), (1320-480)]\n",
    "        dfs = [weekday_stops, weekday_stops, weekday_stops, saturday_stops, sunday_stops]\n",
    "\n",
    "\n",
    "        for i in range(5):\n",
    "\n",
    "            # conditional parameters\n",
    "            df = dfs[i]\n",
    "            minutes = minute_ranges[i]\n",
    "            query = queries[i]\n",
    "\n",
    "            if \"direction_id\" in df.columns:\n",
    "                for direction in [0,1]:\n",
    "                    results = pd.DataFrame(minutes / df[df['direction_id']==direction].query(query)['stop_id'].value_counts())\n",
    "                    results.reset_index(inplace=True)\n",
    "                    results.rename(columns={\"stop_id\":periods[i]}, inplace=True)\n",
    "                    results.rename(columns={\"index\":\"stop_id\"}, inplace=True)\n",
    "\n",
    "                    results[\"stop_id\"] = results[\"stop_id\"].astype(str)\n",
    "                    headways = headways.merge(results, on=\"stop_id\", suffixes=[\"_dir0\", \"_dir1\"], how='left')\n",
    "                \n",
    "            else:\n",
    "                results = pd.DataFrame(minutes / df.query(query)['stop_id'].value_counts())\n",
    "                results.reset_index(inplace=True)\n",
    "                results.rename(columns={\"stop_id\":periods[i]}, inplace=True)\n",
    "                results.rename(columns={\"index\":\"stop_id\"}, inplace=True)\n",
    "\n",
    "                results[\"stop_id\"] = results[\"stop_id\"].astype(str)\n",
    "                headways = headways.merge(results, on=\"stop_id\", how='left')\n",
    "        \n",
    "        if \"am_pk_dir0\" in headways.columns:\n",
    "            headways[\"am_pk\"] = headways[[\"am_pk_dir0\", \"am_pk_dir1\"]].min(axis=1)\n",
    "            headways[\"pm_pk\"] = headways[[\"pm_pk_dir0\", \"pm_pk_dir1\"]].min(axis=1)\n",
    "\n",
    "            headways[\"wkdy\"] = headways[[\"wkdy_dir0\", \"wkdy_dir1\"]].min(axis=1)\n",
    "            headways[\"sat\"] = headways[[\"sat_dir0\", \"sat_dir1\"]].min(axis=1)\n",
    "            headways[\"sun\"] = headways[[\"sun_dir0\", \"sun_dir1\"]].min(axis=1)\n",
    "            headways = headways[['stop_id', \"am_pk\", \"pm_pk\", \"wkdy\", \"sat\", \"sun\"]] \n",
    "        else:\n",
    "            pass\n",
    "        #stops[\"stop_id\"] = stops[\"stop_id\"].astype(str)\n",
    "        output = stops[[\"stop_id\", \"stop_name\", \"stop_lon\", \"stop_lat\"]].merge(headways, on=\"stop_id\", how=\"outer\")\n",
    "\n",
    "        output.to_csv(output_path)\n",
    "    else:\n",
    "        print(\"No bus data.\")\n",
    "    \n",
    "    hqt_filter = \"(am_pk <= 15) & (pm_pk <= 15)\"\n",
    "    hqt_filter += \" & (wkdy <= 20) & \"\n",
    "    hqt_filter += \"(sat <= 30) & (sun <= 30)\"\n",
    "    \n",
    "    \n",
    "    hqt = output.query(hqt_filter)\n",
    "\n",
    "    if len(hqt) > 0:\n",
    "        hqt.to_csv(\"output/v2/bus-hqt/\" + agency_name + \" (HQT).csv\")\n",
    "    else:\n",
    "        pass\n",
    "# output.head()\n",
    "\n",
    "errors_path = \"errors-{}-{}-{}.csv\".format(weekday, saturday, sunday)\n",
    "errors.to_csv(errors_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stops.sort_values(\"arrival_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_stops.drop_duplicates(subset=[\"arrival_time\", \"stop_id\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
